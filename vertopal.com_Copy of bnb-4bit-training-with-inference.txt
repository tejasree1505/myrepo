

transformers meets bitsandbytes for democratzing Large Language Models (LLMs) through 4bit quantization

Welcome to this notebook that goes through the recent bitsandbytes
integration that includes the work from XXX that introduces no
performance degradation 4bit quantization techniques, for democratizing
LLMs inference and training.

In this notebook, we will learn together how to load a large model in
4bit (gpt-neo-x-20b) and train it using Google Colab and PEFT library
from Hugging Face 🤗.

In the general usage notebook, you can learn how to propely load a model
in 4bit with all its variants.

If you liked the previous work for integrating LLM.int8, you can have a
look at the introduction blogpost to lean more about that quantization
method.

    !pip install -q -U bitsandbytes
    !pip install -q -U git+https://github.com/huggingface/transformers.git
    !pip install -q -U git+https://github.com/huggingface/peft.git
    !pip install -q -U git+https://github.com/huggingface/accelerate.git
    !pip install -q datasets

    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/92.2 MB 2.7 MB/s eta 0:00:00
    ents to build wheel ... etadata (pyproject.toml) ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 20.0 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 105.6 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 79.7 MB/s eta 0:00:00
    ers (pyproject.toml) ... ents to build wheel ... etadata (pyproject.toml) ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.1/219.1 kB 4.2 MB/s eta 0:00:00
    l) ... ents to build wheel ... etadata (pyproject.toml) ... l) ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 33.3 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 14.2 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 24.9 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 18.0 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 69.8 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 15.4 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 29.3 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 20.4 MB/s eta 0:00:00

First let's load the model we are going to use - GPT-neo-x-20B! Note
that the model itself is around 40GB in half precision

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

    model_id = "EleutherAI/gpt-neox-20b"
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})

    {"model_id":"0abaefd8bd8d4a67bab6f9ed07881603","version_major":2,"version_minor":0}

    {"model_id":"de48a5b2040e4dcfaaeb8ac023088ab7","version_major":2,"version_minor":0}

    {"model_id":"70295404cf004e438843021799dda7e6","version_major":2,"version_minor":0}

    {"model_id":"f6248b0490d5409f822979d950f57209","version_major":2,"version_minor":0}

    {"model_id":"00fb7eb2d722440eb40004b6e885209a","version_major":2,"version_minor":0}

    {"model_id":"ab476e900ad04a61a06a9ed53af34277","version_major":2,"version_minor":0}

    {"model_id":"a81692c6dca34ed9a418cee05179c82e","version_major":2,"version_minor":0}

    {"model_id":"a2bc403769e743a89047c748172e81ef","version_major":2,"version_minor":0}

    {"model_id":"0f503452676942cb8ed8bf56623bf06e","version_major":2,"version_minor":0}

    {"model_id":"5b5457d892a04dbbb7d12d39eeb383ec","version_major":2,"version_minor":0}

    {"model_id":"5e7c659264b34717b9bac76ee17281c3","version_major":2,"version_minor":0}

    {"model_id":"e889b875d79948f28bbea921898d1f19","version_major":2,"version_minor":0}

    {"model_id":"950151542daf4268b9429a900a30c782","version_major":2,"version_minor":0}

    {"model_id":"410bdf3508074f96bbfef6d64c1e1c36","version_major":2,"version_minor":0}

    {"model_id":"efe5bac3d63c4d6c8f2c54615c4b5932","version_major":2,"version_minor":0}

    {"model_id":"eed3cf99ded74e98bdd90379224af103","version_major":2,"version_minor":0}

    {"model_id":"d2dc748ad4cb4a7ba7a96881453859aa","version_major":2,"version_minor":0}

    {"model_id":"79683eb237e940b5af6e318d1e1ca1e8","version_major":2,"version_minor":0}

    {"model_id":"f20acc97a81c43d9b813ad946e098fba","version_major":2,"version_minor":0}

    {"model_id":"c2c92c4a5e454194a2ffe32ef267a513","version_major":2,"version_minor":0}

    {"model_id":"2a27e067482145e1bda50c3efca7a6bd","version_major":2,"version_minor":0}

    {"model_id":"c6f02c27f0174940bef48c13bf9ef747","version_major":2,"version_minor":0}

    {"model_id":"bf32387fa96042d3becece247dc587b2","version_major":2,"version_minor":0}

    {"model_id":"f15c7f051d2f4bd488cad118d657ddbf","version_major":2,"version_minor":0}

    {"model_id":"6ddb170fbc9c4460bc49d119bb7c78c5","version_major":2,"version_minor":0}

    {"model_id":"ef2cc29453e64d3d8b58071682247cdc","version_major":2,"version_minor":0}

    {"model_id":"17ee1c9adb20409797bea941d4ff0d9e","version_major":2,"version_minor":0}

    {"model_id":"44a002173a254f70b80378771fd87752","version_major":2,"version_minor":0}

    {"model_id":"544ff918ac5a408eb47c6194601a2717","version_major":2,"version_minor":0}

    {"model_id":"c69b357ed42f46b191ea6aa30a5946c4","version_major":2,"version_minor":0}

    {"model_id":"6b9b1e4ab6f94bc594b6aa6846dff6b2","version_major":2,"version_minor":0}

    {"model_id":"290f9f02020a4e4e89eefc579e9d0f14","version_major":2,"version_minor":0}

    {"model_id":"f7ae915ebfba4507a69ffbce4cea643a","version_major":2,"version_minor":0}

    {"model_id":"647450c4fa7843ca98d0157fe6b56617","version_major":2,"version_minor":0}

    {"model_id":"789358850a7f40499f3f47efd82123ca","version_major":2,"version_minor":0}

    {"model_id":"780fe0041e124d0c92e9d099e0aea0ec","version_major":2,"version_minor":0}

    {"model_id":"f2f0e1ea1d5a4960afb533443e76f262","version_major":2,"version_minor":0}

    {"model_id":"8804ee09effe44cd93a37b65d252e9d7","version_major":2,"version_minor":0}

    {"model_id":"8d711780c52f476589e68ce166dc4ab4","version_major":2,"version_minor":0}

    {"model_id":"544a2ba68a404688bf8417159f1e1c9a","version_major":2,"version_minor":0}

    {"model_id":"146c0f1356924fe0a2c44bae469734e4","version_major":2,"version_minor":0}

    {"model_id":"a6fa105c0cb249b78a332c05c59b7159","version_major":2,"version_minor":0}

    {"model_id":"a7fc1c9f3793457492ef0b144b1bd2d4","version_major":2,"version_minor":0}

    {"model_id":"154d094d1a75424684b17a87fa2552ce","version_major":2,"version_minor":0}

    {"model_id":"68a04b4b5340477a9d53da369d4be56a","version_major":2,"version_minor":0}

    {"model_id":"217ef8035de94d448c9654aa0adb4ce5","version_major":2,"version_minor":0}

    {"model_id":"a1ab79eb5daf4d0e97778098d8e7a284","version_major":2,"version_minor":0}

    {"model_id":"cd33e4c1538e4b8a8b1f74da59360b7b","version_major":2,"version_minor":0}

    {"model_id":"4731d26f4d2d42088ac11b27830f65ce","version_major":2,"version_minor":0}

    {"model_id":"834234f4e2da4718b19ea95302862c9f","version_major":2,"version_minor":0}

    {"model_id":"85267bca8a77442fbed7702deca0df58","version_major":2,"version_minor":0}

    {"model_id":"f3ed071450da4ac08ce22c157a8e4b37","version_major":2,"version_minor":0}

    {"model_id":"ba747c6af83142ddafdd31d48faba0fe","version_major":2,"version_minor":0}

    {"model_id":"30a67537526b4f349af4f0f3e83bcf42","version_major":2,"version_minor":0}


    ===================================BUG REPORT===================================
    Welcome to bitsandbytes. For bug reports, please run

    python -m bitsandbytes

     and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
    ================================================================================
    bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so
    CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
    CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0
    CUDA SETUP: Highest compute capability among GPUs detected: 7.5
    CUDA SETUP: Detected CUDA version 118
    CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...

    /usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
      warn(msg)
    /usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}
      warn(msg)
    /usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}
      warn(msg)
    /usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1efnc3smi84id --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}
      warn(msg)
    /usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}
      warn(msg)
    /usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}
      warn(msg)
    /usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
    Either way, this might cause trouble in the future:
    If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
      warn(msg)

    {"model_id":"9196d4b349e6476499c120b11f1607fa","version_major":2,"version_minor":0}

Then we have to apply some preprocessing to the model to prepare it for
training. For that use the prepare_model_for_kbit_training method from
PEFT.

    from peft import prepare_model_for_kbit_training

    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)

    def print_trainable_parameters(model):
        """
        Prints the number of trainable parameters in the model.
        """
        trainable_params = 0
        all_param = 0
        for _, param in model.named_parameters():
            all_param += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
        print(
            f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
        )

    from peft import LoraConfig, get_peft_model

    config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["query_key_value"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, config)
    print_trainable_parameters(model)

    trainable params: 8650752 || all params: 10597552128 || trainable%: 0.08162971878329976

Let's load a common dataset, english quotes, to fine tune our model on
famous quotes.

    from datasets import load_dataset

    data = load_dataset("Abirate/english_quotes")
    data = data.map(lambda samples: tokenizer(samples["quote"]), batched=True)

    {"model_id":"66d4e1db7ffb4ddfa44751146941499d","version_major":2,"version_minor":0}

    Downloading and preparing dataset json/Abirate--english_quotes to /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...

    {"model_id":"f06ab120a0d64e198f9be77f67986003","version_major":2,"version_minor":0}

    {"model_id":"b27d5a7ef8704610a90c76eb26799918","version_major":2,"version_minor":0}

    {"model_id":"f79e95663d4442ac8ff2b7f6f054708f","version_major":2,"version_minor":0}

    {"model_id":"9db84319d60d43538d79bfb536dc2eb5","version_major":2,"version_minor":0}

    Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.

    {"model_id":"60acefedfd7a46719945db3ad75c3dad","version_major":2,"version_minor":0}

    {"model_id":"f61f1183e5d946b19dcaee35d33fb7a7","version_major":2,"version_minor":0}

Run the cell below to run the training! For the sake of the demo, we
just ran it for few steps just to showcase how to use this integration
with existing tools on the HF ecosystem.

    import transformers

    # needed for gpt-neo-x tokenizer
    tokenizer.pad_token = tokenizer.eos_token

    trainer = transformers.Trainer(
        model=model,
        train_dataset=data["train"],
        args=transformers.TrainingArguments(
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=2,
            max_steps=10,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=1,
            output_dir="outputs",
            optim="paged_adamw_8bit"
        ),
        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
    )
    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
    trainer.train()

    You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
    /usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:229: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)
      attn_scores = torch.where(causal_mask, attn_scores, mask_value)

    <IPython.core.display.HTML object>

    TrainOutput(global_step=10, training_loss=2.434360909461975, metrics={'train_runtime': 169.887, 'train_samples_per_second': 0.235, 'train_steps_per_second': 0.059, 'total_flos': 99255709532160.0, 'train_loss': 2.434360909461975, 'epoch': 0.02})

    model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training
    model_to_save.save_pretrained("outputs")

    lora_config = LoraConfig.from_pretrained('outputs')
    model = get_peft_model(model, lora_config)

    text = "Elon Musk "
    device = "cuda:0"

    inputs = tokenizer(text, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=20)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

    Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.

    Elon Musk 
    Elon Musk is a South African-born Canadian-American business magnate, investor, engineer
