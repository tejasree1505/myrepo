We are leveraging generative AI, to help customers gain deeper insights into the predicted values of the target variables mentioned already. This approach allows for more dynamic and contextually relevant explanations ^^compared to traditional static analysis.^^ The insights generated based on predicted financial data, helps the customers in better decision-making.

For generating the inference on the predicted financial data, we used hugging face's GPT-2 model. It is a decoder-only model with 1.5 billion parameters. Its architecture is based on the Transformer model, which uses self-attention mechanisms to handle long-range dependencies in text effectively. ^^This makes it particularly suitable for generating coherent and contextually accurate text, which is essential for this use case.^^
GPT-2's architecture consists of multiple layers of transformers, each with attention heads that allow the model to focus on different parts of the input text. This layered approach enables it to understand and generate detailed responses, which is ideal for answering complex financial questions.

The reason for choosing gpt2 over other available models is that While BERT and FlanT5 are strong in understanding and transforming text, they are primarily designed for tasks like classification and translation rather than generation.
Comparatively, BART is a seq2seq model suitable for text generation but is more complex and computationally intensive. GPT-2, on the other hand, strikes a balance between performance and computational efficiency. ^^Its autoregressive nature allows it to generate text in a fluent and coherent manner, which is crucial for our goal of providing meaningful insights into financial ratios.^^

Even though the GPT-2 model is trained on a vast amount of diverse text data, its generic pre-training does not provide the desired level of relevance while providing insights. Therefore the model has to be finetuned on financial data. Since gpt2 is a decoder-only model, it is usually used to generate the next token. But when we are trying to get inference based on the given financial data, we are focusing on doing a sequence-to-sequence task. So for making it do sequence-to-sequence tasks, it needs to be fine-tuned with a question-answer dataset.
The dataset we used to finetune contains questions related to the current ratio, inventory turnover ratio, and receivables turnover ratio. This ensures the model is exposed to diverse examples, enhancing its ability to generate accurate and contextually relevant responses.
--
There are few concerns while finetuning: We had to be very careful with fine-tuning because of catastrophic forgetting. In essence, after we finetune our model, we didn’t want to overwrite the original weights so much that they forget previously learned connections. 
LoRA is a parameter-efficient fine tuning technique. The central idea of LoRA is that you should keep the original pre-trained weights, meaning that the original pre trained weights are frozen and add some new low-parameter weights to fine-tune instead. These low-parameter weights are added to the pre-trained weights and when training, we only update the new weights, thus reducing the training time and memory used and computational cost. LoRA shrinks the difficulty of training and fine-tuning large language models (LLMs) by reducing the number of trainable parameters and producing lightweight and efficient models. There were only 0.31% trainable parameters. Also due to our small dataset size,  changing all the model's weight won't give effective results.
The small downside is that the extra weights add to the overall memory needed at inference time, though just by a small percentage.
---
After generating insights with the finetuned GPT-2 model, we feed the inferences into FinBERT for sentiment analysis. FinBERT is a BERT-based model specifically trained for financial text, enabling it to understand the sentiment behind financial statements effectively. We further fine-tuned it on the target features to provide more accuracy.
This integration allows us to not only provide detailed explanations of financial ratios but also analyze the sentiment associated with the aggregated explanation. 
The inference of all 3 target variables are concatenated and, FinBERT can assess whether this is perceived positively or negatively in the financial context. This added layer of analysis enhances the overall insight provided to users. ^^It enables a deeper understanding of how financial metrics impact stakeholder sentiment, which is crucial for comprehensive financial analysis.^^
---
Genai Workflow *
While our finetuned model shows good results, it’s important to note that it doesn't always provide perfect answers. This prototype demonstrates the potential of generative AI in financial analysis. For more accurate and reliable inferences, advanced models like LLaMA could be employed in the future, offering improved performance and consistency.
---
