We are leveraging generative AI, to help customers gain deeper insights into the predicted values of the target variables mentioned already. This approach allows for more dynamic and contextually relevant explanations compared to traditional static analysis. The generative AI can adapt to various scenarios, providing tailored insights based on predicted financial data, thus aiding the customers in better decision-making.

For generating the inference on the predicted financial data, we used hugging face's GPT-2 model. It is a decoder-only model with 1.5 billion parameters. Its architecture is based on the Transformer model, which uses self-attention mechanisms to handle long-range dependencies in text effectively. This makes it particularly suitable for generating coherent and contextually accurate text, which is essential for this use case.
GPT-2's architecture consists of multiple layers of transformers, each with attention heads that allow the model to focus on different parts of the input text. This layered approach enables it to understand and generate detailed and nuanced responses, which is ideal for answering complex financial questions.

We chose GPT-2 over other models like BART, BERT, and FlanT5 for several reasons. GPT-2 excels in generative tasks, making it ideal for generating detailed explanations. While BERT and FlanT5 are strong in understanding and transforming text, they are primarily designed for tasks like classification and translation rather than generation.
Comparatively, BART is a seq2seq model suitable for text generation but is more complex and computationally intensive. GPT-2, on the other hand, strikes a balance between performance and computational efficiency. Its autoregressive nature allows it to generate text in a fluent and coherent manner, which is crucial for our goal of providing meaningful insights into financial ratios.

Even though the GPT-2 model is trained on a vast amount of diverse text data, its generic pre-training does not provide the desired level of relevance while providing insights. Therefore the model has to be finetuned on financial data. Since gpt2 is a decoder-only model, it is usually used to generate the next token. But when we are trying to get inference based on the given financial data, we are focusing on doing a sequence-to-sequence task. So for making it do sequence-to-sequence tasks, it needs to be fine-tuned with a question-answer dataset.
The Q/A dataset was meticulously curated to include various scenarios related to the current ratio, inventory turnover ratio, and receivables turnover ratio. This ensures the model is exposed to diverse examples, enhancing its ability to generate accurate and contextually relevant responses.

There are few concerns while finetuning: We had to be very careful with fine-tuning because of catastrophic forgetting. In essence, after we finetune our model, we didn’t want to overwrite the original weights so much that they forget previously learned connections. 
LoRA is a parameter-efficient fine tuning technique. The central idea of LoRA is that you should keep the original pre-trained weights and add some new low-parameter weights to fine-tune instead. These low-parameter weights are added to the pre-trained weights and when training, we only update the new weights, thus reducing the training time and memory used and computational cost. LoRA shrinks the difficulty of training and fine-tuning large language models (LLMs) by reducing the number of trainable parameters and producing lightweight and efficient models. There were only 0.31% trainable parameters.
The small downside is that the extra weights add to the overall memory needed at inference time, though just by a small percentage.

After generating insights with the finetuned GPT-2 model, we feed the inferences into FinBERT for sentiment analysis. FinBERT is a BERT-based model specifically trained for financial text, enabling it to understand the sentiment behind financial statements effectively. We further fine-tuned it on the target features to provide more accuracy.
This integration allows us to not only provide detailed explanations of financial ratios but also analyze the sentiment associated with these explanations. For example, if the GPT-2 model explains an increase in the current ratio, FinBERT can assess whether this change is perceived positively or negatively in the financial context. This added layer of analysis enhances the overall insight provided to users. It enables a deeper understanding of how financial metrics impact stakeholder sentiment, which is crucial for comprehensive financial analysis.

While our finetuned model shows promising results, it’s important to note that it doesn't always provide perfect answers. This prototype demonstrates the potential of generative AI in financial analysis. For more accurate and reliable inferences, models like LLaMA could be employed in the future, offering improved performance and consistency.

extras::
r (attention heads): This parameter defines the number of attention heads used in the transformer layers. Attention heads allow the model to focus on different parts of the input sequence, enabling it to capture various aspects of the context.

lora_alpha (alpha scaling): This parameter controls the scaling factor for the LoRA (Low-Rank Adaptation) layers. It helps in adjusting the magnitude of the learned low-rank updates, ensuring they effectively complement the pre-trained model weights.


